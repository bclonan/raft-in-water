{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Skip Connections\n",
    "\n",
    "1.3. Skip Connection.\n",
    "\n",
    "Skip Connection is a standard module in many convolutional architectures. By using Skip Connection, we provide an alternative path for the gradient (with backpropagation). These additional paths are beneficial for model convergence.\n",
    "\n",
    "Skip Connection: skip some layer in neural network and feeds the output of one layer as the input to the next layers (instead of only the next one).\n",
    "\n",
    "When using the chain rule, we must keep multiplying terms with the error gradient as we go backward. However, in long chain of multiplication, if we multiply many things together that are less than 1, the resulting gradient will be very small. Therefore, the gradient becomes very small as we approach the earlier layers in a deep architecture. In some cases, the gradient becomes zero, meaning that we do not update the early layers at all.\n",
    "\n",
    "There are two fundamental ways that we could use Skip Connections through different non-sequential layers:\n",
    "\n",
    "Addition as in residual architectures.\n",
    "Concatenation as in densely connected architecture.\\\n",
    "\n",
    "1\n",
    "What are skip connections?\n",
    "Skip connections are a type of shortcut that connects the output of one layer to the input of another layer that is not adjacent to it. For example, in a CNN with four layers, A, B, C, and D, a skip connection could connect layer A to layer C, or layer B to layer D, or both. Skip connections can be implemented in different ways, such as adding, concatenating, or multiplying the outputs of the skipped layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a test similar to the one described involves several steps, encompassing the design of the neural network architecture, implementing skip connections, using a specific activation function, applying L2 regularization, and introducing rotation transformations during training. Below is a high-level guide on how to set up and conduct such an experiment using Python and a popular deep learning library like TensorFlow or PyTorch.\n",
    "\n",
    "### Step 1: Environment Setup\n",
    "\n",
    "Ensure you have Python installed on your machine along with TensorFlow or PyTorch. This guide will use TensorFlow for illustration purposes, but the concepts are transferable to PyTorch with similar functionalities.\n",
    "\n",
    "```bash\n",
    "pip install tensorflow numpy\n",
    "```\n",
    "\n",
    "### Step 2: Create the Neural Network Model\n",
    "\n",
    "Define a neural network model with three layers, using skip connections and the `tanh` activation function. TensorFlow's functional API can handle skip connections easily.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Define three layers with skip connections\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    skip1 = layers.Add()([x, inputs])\n",
    "\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip1)\n",
    "    skip2 = layers.Add()([x, skip1])\n",
    "\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip2)\n",
    "    output = layers.Add()([x, skip2])\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "```\n",
    "\n",
    "### Step 3: Training Setup\n",
    "\n",
    "Prepare your dataset. Since this experiment involves learning from random data, you can generate synthetic data pairs `(x, y)` where both `x` and `y` are random vectors of the same dimension.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Generate random training data\n",
    "num_samples = 1000\n",
    "input_shape = 20  # Example input shape\n",
    "x_train = np.random.rand(num_samples, input_shape)\n",
    "y_train = np.random.rand(num_samples, input_shape)\n",
    "```\n",
    "\n",
    "### Step 4: Initial Training\n",
    "\n",
    "Train the model using gradient descent, typically by compiling and fitting the model in TensorFlow.\n",
    "\n",
    "```python\n",
    "model = create_model(input_shape)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "### Step 5: Apply Rotation Transformations\n",
    "\n",
    "Implement a custom training loop where you apply the rotation transformations to the weight matrices after each training step. The rotation transformation can be applied as described, using small random anti-symmetric matrices `H_i`.\n",
    "\n",
    "```python\n",
    "def apply_rotation(W, H):\n",
    "    # Example rotation transformation implementation\n",
    "    return W + np.dot(H, W) - np.dot(W, H)\n",
    "\n",
    "# Custom training loop to apply rotation\n",
    "for epoch in range(10):  # Additional training epochs\n",
    "    # Standard training step\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=32, verbose=0)\n",
    "\n",
    "    # Apply rotation transformations to each layer's weights\n",
    "    for layer in model.layers:\n",
    "        if 'dense' in layer.name:  # Targeting only Dense layers\n",
    "            W = layer.get_weights()[0]  # Get current weights\n",
    "            H = np.random.rand(*W.shape)  # Generate H matrix\n",
    "            H = H - H.T  # Make H anti-symmetric\n",
    "            W_rotated = apply_rotation(W, H * 0.01)  # Small rotation\n",
    "            layer.set_weights([W_rotated, layer.get_weights()[1]])  # Update weights\n",
    "```\n",
    "\n",
    "### Step 6: Evaluation and Interpretation\n",
    "\n",
    "After training, evaluate the model's performance on test data or further analyze the model's behavior and weight changes due to rotation transformations. This process involves looking at the loss and accuracy metrics or using visualization tools to understand the weight matrix dynamics over training epochs.\n",
    "\n",
    "This experiment is a simplified illustration. The actual implementation, especially the rotation transformation part, would need more nuanced handling to closely mimic the quantum mechanical rotation analogy and to maintain the network's stability and performance as described.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Define three layers with skip connections\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    skip1 = layers.Add()([x, inputs])\n",
    "    \n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip1)\n",
    "    skip2 = layers.Add()([x, skip1])\n",
    "    \n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip2)\n",
    "    output = layers.Add()([x, skip2])\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research proj two breaking it down step by step\n",
    "\n",
    "Experiment: Infinitely Woven Skip Connections\n",
    "Goal: Develop a novel skip connection technique that allows for infinitely weaving connections between a base grid and the input/output of a neural network.\n",
    "\n",
    "Experiment Design:\n",
    "\n",
    "1. Base Grid:\n",
    "\n",
    "Create a constant base grid of integers with dimension 100 x 100.\n",
    "Each element in the grid represents a unique identifier for that specific location. 2. Neural Network:\n",
    "\n",
    "Define a neural network architecture suitable for your task, taking the original 100 x 100 integer matrix as input.\n",
    "The network's output can be either:\n",
    "A single vector of size 100 x 100 (corresponding to each element in the input matrix).\n",
    "A matrix of the same dimension (100 x 100) containing different values for each element. 3. Infinitely Woven Skip Connections:\n",
    "\n",
    "Weaving Unit: Design a weaving unit that takes three inputs:\n",
    "Base Grid Element: A single element from the base grid (integer).\n",
    "Network Input: The corresponding element from the original input matrix (integer).\n",
    "Network Output: The corresponding element from the network's output (either a scalar or a vector element, depending on the chosen output format).\n",
    "The weaving unit can perform various operations on the input elements depending on your specific needs. Examples include:\n",
    "Concatenation: Combine the base grid element, network input, and network output into a single vector.\n",
    "Gated fusion: Use a gating mechanism to selectively incorporate information from the network input and output while respecting the base grid element.\n",
    "Learned transformation: Apply a learned transformation on the concatenated information (or any other combination) to generate the final output.\n",
    "The weaving unit's output becomes the final \"woven\" representation for that element. 4. Output Layer:\n",
    "\n",
    "Create an output layer with the same dimension as the input/output (100 x 100).\n",
    "Each element in the output layer is obtained by applying the weaving unit to the corresponding element in the base grid, its corresponding element in the network input, and its corresponding element in the network output. 5. Training:\n",
    "\n",
    "Define a cost function based on your specific task and optimize the entire network, including the weaving unit, using gradient-based optimization algorithms (e.g., Adam, SGD).\n",
    "Evaluation:\n",
    "\n",
    "Compare the performance of the proposed \"infinitely woven skip connection\" approach against a baseline model without skip connections and a model with standard skip connections (e.g., residual connections) on the same task and data.\n",
    "Analyze the impact of different weaving unit designs on performance and efficiency.\n",
    "Visualize or interpret the learned weights/parameters within the weaving unit to understand how the connections are being formed and utilized.\n",
    "Benefits:\n",
    "\n",
    "This approach allows for potentially richer information flow within the network by weaving connections between the base grid, initial input, and learned features throughout the network.\n",
    "The concept of \"infinitely woven connections\" offers flexibility in how information from different parts of the network is incorporated.\n",
    "Challenges:\n",
    "\n",
    "Designing an efficient and effective weaving unit architecture is crucial.\n",
    "Training the network with an additional layer of complexity might require careful hyperparameter tuning and potentially more data.\n",
    "The interpretability of the learned \"woven\" representations might require further investigation.\n",
    "Future Work:\n",
    "\n",
    "Explore different weaving unit architectures and their impact on performance and interpretability.\n",
    "Investigate the application of this technique to different tasks and network architectures.\n",
    "Analyze the computational efficiency of this approach compared to standard skip connections.\n",
    "This outline provides a starting point for your experiment. You can adapt the specific details of the neural network architecture, weaving unit design, and evaluation metrics to fit your specific needs and research question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Setup for a Similar Project on Skip Connections and ResNet\n",
    "Prerequisites:\n",
    "Python 3.6 or later\n",
    "TensorFlow 2.0 or later\n",
    "PyTorch (optional)\n",
    "Materials:\n",
    "ResNet code from Medium article\n",
    "Keras implementation of ResNet\n",
    "Steps:\n",
    "1. Data Preparation\n",
    "Obtain the CIFAR-10 dataset or another image classification dataset.\n",
    "Preprocess the data by resizing, normalization, and splitting into training and validation sets.\n",
    "2. Model Architecture\n",
    "Implement the ResNet architecture using either TensorFlow or PyTorch.\n",
    "Define the skip connection paths as described in the Medium article.\n",
    "3. Training and Evaluation\n",
    "Train the model on the training set using an optimizer and loss function.\n",
    "Evaluate the model's performance on the validation set.\n",
    "Experiment with different hyperparameters, such as learning rate and batch size, to optimize performance.\n",
    "4. Visualization and Analysis\n",
    "Plot the training and validation loss curves to monitor the model's progress.\n",
    "Use tensorboard to visualize the model's layers and activations.\n",
    "Conduct ablation studies to analyze the impact of skip connections on the model's performance.\n",
    "Additional Features:\n",
    "Pre-trained models: Utilize pre-trained ResNet models for feature extraction or fine-tuning.\n",
    "Data augmentation: Enhance the data variability by applying transformations such as cropping, flipping, and rotation.\n",
    "Batch normalization: Improve model stability and convergence.\n",
    "Tips:\n",
    "Start with a simple ResNet architecture with a few layers to ensure understanding before scaling up.\n",
    "Use a GPU for faster training and inference.\n",
    "Explore other variants of ResNet, such as ResNeXt or Wide ResNet.\n",
    "Extension:\n",
    "Extend the project by exploring the use of skip connections in other deep neural network architectures, such as U-Nets or transformers.\n",
    "Investigate the theoretical aspects of skip connections, such as their role in preventing vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow keras numpy matplotlib scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 0.1\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Define three layers with skip connections\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    skip1 = layers.Add()([x, inputs])\n",
    "    \n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip1)\n",
    "    skip2 = layers.Add()([x, skip1])\n",
    "    \n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip2)\n",
    "    output = layers.Add()([x, skip2])\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape=(1,))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=0)\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the model's layers and activations using tensorboard\n",
    "# Load the TensorBoard notebook extension\n",
    "load_ext tensorboard\n",
    "# Start TensorBoard within the notebook using magics\n",
    "tensorboard --logdir logs\n",
    "\n",
    "# Conduct ablation studies to analyze the impact of skip connections on the model's performance\n",
    "# Define a function to create the model with or without skip connections\n",
    "\n",
    "def create_model(input_shape, use_skip_connections=True):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Define three layers with or without skip connections\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    if use_skip_connections:\n",
    "        skip1 = layers.Add()([x, inputs])\n",
    "        x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip1)\n",
    "        skip2 = layers.Add()([x, skip1])\n",
    "        x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip2)\n",
    "        output = layers.Add()([x, skip2])\n",
    "    else:\n",
    "        x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        output = layers.Dense(1)(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Train the model with skip connections\n",
    "model_with_skip_connections = create_model(input_shape=(1,))\n",
    "model_with_skip_connections.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history_with_skip_connections = model_with_skip_connections.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=0)\n",
    "\n",
    "# Train the model without skip connections\n",
    "model_without_skip_connections = create_model(input_shape=(1,), use_skip_connections=False)\n",
    "model_without_skip_connections.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history_without_skip_connections = model_without_skip_connections.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=0)\n",
    "\n",
    "# Plot the training and validation loss curves for both models\n",
    "plt.plot(history_with_skip_connections.history['loss'], label='Training Loss (with skip connections)')\n",
    "plt.plot(history_with_skip_connections.history['val_loss'], label='Validation Loss (with skip connections)')\n",
    "plt.plot(history_without_skip_connections.history['loss'], label='Training Loss (without skip connections)')\n",
    "plt.plot(history_without_skip_connections.history['val_loss'], label='Validation Loss (without skip connections)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the model's training history to a CSV file\n",
    "history_df = pd.DataFrame({'Training Loss (with skip connections)': history_with_skip_connections.history['loss'],\n",
    "                           'Validation Loss (with skip connections)': history_with_skip_connections.history['val_loss'],\n",
    "                           'Training Loss (without skip connections)': history_without_skip_connections.history['loss'],\n",
    "                           'Validation Loss (without skip connections)': history_without_skip_connections.history['val_loss']})\n",
    "history_df.to_csv('model_training_history.csv', index=False)\n",
    "\n",
    "# Load the model's training history from the CSV file\n",
    "loaded_history_df = pd.read_csv('model_training_history.csv')\n",
    "print(loaded_history_df)\n",
    "\n",
    "# Use pre-trained ResNet models for feature extraction or fine-tuning\n",
    "# Load the pre-trained ResNet50 model\n",
    "resnet_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Extract features from the pre-trained ResNet model\n",
    "features = resnet_model.predict(X_train)\n",
    "print(features.shape)\n",
    "\n",
    "# Fine-tune the pre-trained ResNet model\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "model = models.Sequential([\n",
    "    resnet_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "# Enhance the data variability by applying transformations such as cropping, flipping, and rotation\n",
    "# Define data augmentation layers\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Visualize the augmented images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "# Improve model stability and convergence using batch normalization\n",
    "# Define the model architecture with batch normalization\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=10)\n",
    "\n",
    "# Explore other variants of ResNet, such as ResNeXt or Wide ResNet\n",
    "# Load the ResNeXt50 model\n",
    "resnext_model = tf.keras.applications.ResNeXt50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Load the Wide ResNet50 model\n",
    "wideresnet_model = tf.keras.applications.WideResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Explore the use of skip connections in other deep neural network architectures, such as U-Nets or transformers\n",
    "\n",
    "# Investigate the theoretical aspects of skip connections, such as their role in preventing vanishing gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This function simply resizes the images to fit in AlexNet\n",
    "#  Copyright 2017 The MathWorks, Inc.\n",
    "function I = readFunctionTrain(filename)\n",
    "# Resize the images to the size required by the network.\n",
    "I = imread(filename);\n",
    "I = imresize(I, [227 227]);\n",
    "\n",
    "function saveCIFAR10AsFolderOfImages(inputPath, outputPath, varargin)\n",
    " saveCIFAR10AsFolderOfImages   Save the CIFAR-10 dataset as a folder of images\n",
    "   saveCIFAR10AsFolderOfImages(inputPath, outputPath) takes the CIFAR-10\n",
    "   dataset located at inputPath and saves it as a folder of images to the\n",
    "   directory outputPath. If inputPath or outputPath is an empty string, it\n",
    "   is assumed that the current folder should be used.\n",
    "\n",
    "   saveCIFAR10AsFolderOfImages(..., labelDirectories) will save the\n",
    "   CIFAR-10 data so that instances with the same label will be saved to\n",
    "   sub-directories with the name of that label.\n",
    "    Check input directories are valid\n",
    "if(~isempty(inputPath))\n",
    "    assert(exist(inputPath,'dir') == 7);\n",
    "end\n",
    "if(~isempty(outputPath))\n",
    "    assert(exist(outputPath,'dir') == 7);\n",
    "end\n",
    " Check if we want to save each set with the same labels to its own\n",
    " directory.\n",
    "if(isempty(varargin))\n",
    "    labelDirectories = false;\n",
    "else\n",
    "    assert(nargin == 3);\n",
    "    labelDirectories = varargin{1};\n",
    "end\n",
    " Set names for directories\n",
    "trainDirectoryName = 'cifar10Train';\n",
    "testDirectoryName = 'cifar10Test';\n",
    " Create directories for the output\n",
    "mkdir(fullfile(outputPath, trainDirectoryName));\n",
    "mkdir(fullfile(outputPath, testDirectoryName));\n",
    "if(labelDirectories)\n",
    "    labelNames = {'airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck'};\n",
    "    iMakeTheseDirectories(fullfile(outputPath, trainDirectoryName), labelNames);\n",
    "    iMakeTheseDirectories(fullfile(outputPath, testDirectoryName), labelNames);\n",
    "    for i = 1:5\n",
    "        iLoadBatchAndWriteAsImagesToLabelFolders(fullfile(inputPath,['data_batch_' num2str(i) '.mat']), fullfile(outputPath, trainDirectoryName), labelNames, (i-1)*10000);\n",
    "    end\n",
    "    iLoadBatchAndWriteAsImagesToLabelFolders(fullfile(inputPath,'test_batch.mat'), fullfile(outputPath, testDirectoryName), labelNames, 0);\n",
    "else\n",
    "    for i = 1:5\n",
    "        iLoadBatchAndWriteAsImages(fullfile(inputPath,['data_batch_' num2str(i) '.mat']), fullfile(outputPath, trainDirectoryName), (i-1)*10000);\n",
    "    end\n",
    "    iLoadBatchAndWriteAsImages(fullfile(inputPath,'test_batch.mat'), fullfile(outputPath, testDirectoryName), 0);\n",
    "end\n",
    "end\n",
    "function iLoadBatchAndWriteAsImagesToLabelFolders(fullInputBatchPath, fullOutputDirectoryPath, labelNames, nameIndexOffset)\n",
    "load(fullInputBatchPath);\n",
    "data = data'; #ok<NODEF>\n",
    "data = reshape(data, 32,32,3,[]);\n",
    "data = permute(data, [2 1 3 4]);\n",
    "for i = 1:size(data,4)\n",
    "    imwrite(data(:,:,:,i), fullfile(fullOutputDirectoryPath, labelNames{labels(i)+1}, ['image' num2str(i + nameIndexOffset) '.png']));\n",
    "end\n",
    "end\n",
    "function iLoadBatchAndWriteAsImages(fullInputBatchPath, fullOutputDirectoryPath, nameIndexOffset)\n",
    "load(fullInputBatchPath);\n",
    "data = data'; #ok<NODEF>\n",
    "data = reshape(data, 32,32,3,[]);\n",
    "data = permute(data, [2 1 3 4]);\n",
    "for i = 1:size(data,4)\n",
    "    imwrite(data(:,:,:,i), fullfile(fullOutputDirectoryPath, ['image' num2str(i + nameIndexOffset) '.png']));\n",
    "end\n",
    "end\n",
    "function iMakeTheseDirectories(outputPath, directoryNames)\n",
    "for i = 1:numel(directoryNames)\n",
    "    mkdir(fullfile(outputPath, directoryNames{i}));\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images/255.0\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate a 100x100 grid of random integers (for demonstration purposes)\n",
    "# Let's assume values range from 1 to 100 for simplicity\n",
    "grid = np.random.randint(1, 101, size=(100, 100))\n",
    "\n",
    "# Perform a simplified \"transform\" by summing values along one dimension\n",
    "# This is akin to reducing the dimensionality of the data for further analysis\n",
    "transformed_data = np.sum(grid, axis=0)  # Sum along columns\n",
    "\n",
    "# For demonstration, let's show the shape of the original and transformed data\n",
    "original_shape = grid.shape\n",
    "transformed_shape = transformed_data.shape\n",
    "\n",
    "original_shape, transformed_shape, transformed_data[:10]  # Display shapes and first 10 values of transformed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "# Generate a 100x100 grid of random integers (for demonstration purposes)\n",
    "# Let's assume values range from 1 to 100 for simplicity\n",
    "grid = base_grid\n",
    "\n",
    "# Perform a simplified \"transform\" by summing values along one dimension\n",
    "# This is akin to reducing the dimensionality of the data for further analysis\n",
    "transformed_data = np.sum(grid, axis=0)  # Sum along columns\n",
    "\n",
    "# For demonstration, let's show the shape of the original and transformed data\n",
    "original_shape = grid.shape\n",
    "transformed_shape = transformed_data.shape\n",
    "\n",
    "original_shape, transformed_shape, transformed_data[:10]  # Display shapes and first 10 values of transformed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periodic summation is a concept from mathematics that allows for the extension of an integrable function \\(s(t)\\) into a periodic function \\(s_P(t)\\) with a period \\(P\\). This is achieved by summing shifted copies of the original function \\(s(t)\\) at intervals that are integer multiples of \\(P\\). Mathematically, this is represented as:\n",
    "\n",
    "\\[s*P(t) = \\sum*{n=-\\infty}^{\\infty} s(t + nP)\\]\n",
    "\n",
    "This technique is pivotal in signal processing and analysis, particularly when dealing with Fourier transforms, as it allows for the transformation of non-periodic signals into periodic ones. The Fourier coefficients of the periodically summed function \\(s_P(t)\\) correspond to the values of the continuous Fourier transform of \\(s(t)\\) at intervals of \\(\\frac{1}{P}\\). This relationship is an instance of the Poisson summation formula.\n",
    "\n",
    "To illustrate periodic summation and its effects, let's simulate a simple integrable function and apply periodic summation to it. We'll use a discretized approach suitable for computational demonstration. Specifically, we will:\n",
    "\n",
    "1. Define a simple integrable function \\(s(t)\\) over a discrete set of points.\n",
    "2. Apply periodic summation to create a periodic function \\(s_P(t)\\) with a chosen period \\(P\\).\n",
    "3. Visualize the original and periodically summed functions for comparison.\n",
    "\n",
    "Let's start by defining a simple function \\(s(t)\\) and then perform periodic summation on it.\n",
    "\n",
    "In the demonstration above, we've illustrated the concept of periodic summation with a simple function \\(s(t) = e^{-t}\\) over an interval \\([0, 5)\\) and applied periodic summation to extend it beyond its original domain, making it periodic with a period \\(P = 5\\).\n",
    "\n",
    "- **Original Function \\($s(t)$):** The left plot shows the original function \\(s(t) = e^{-t}\\), defined over the interval \\([0, 5)\\). This function decays exponentially and is not inherently periodic.\n",
    "- **Periodically Summed Function \\($s_P(t)$):** The right plot displays the result of applying periodic summation to \\(s(t)\\), effectively replicating and shifting the function to create a periodic extension. For visualization purposes, we've replicated the function three times, but theoretically, the summation extends infinitely in both directions.\n",
    "\n",
    "This process demonstrates how periodic summation transforms a non-periodic function into a periodic one, a principle that underpins various applications in signal processing and Fourier analysis. The resultant periodic function can then be analyzed using tools like Fourier series, revealing insights about the original signal's frequency components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a simple function s(t) = exp(-t) for t in [0, 5)\n",
    "t = np.linspace(0, 5, 1000, endpoint=False)\n",
    "s_t = np.exp(-t)\n",
    "\n",
    "# Choose a period P for the summation\n",
    "P = 5\n",
    "# Extended t for periodic summation\n",
    "t_extended = np.linspace(-P, 2*P, 3000, endpoint=False)\n",
    "\n",
    "# Perform periodic summation: for simplicity in a discrete setting, we replicate and shift the function\n",
    "s_P_t = np.tile(s_t, 3)  # Replicate s(t) three times for demonstration\n",
    "\n",
    "# Plot the original and periodically summed function\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Original function plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, s_t, label='$s(t) = e^{-t}$')\n",
    "plt.title('Original Function $s(t)$')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('$s(t)$')\n",
    "plt.legend()\n",
    "\n",
    "# Periodically summed function plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(t_extended, s_P_t, label='$s_P(t)$ with $P=5$')\n",
    "plt.title('Periodically Summed Function $s_P(t)$')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('$s_P(t)$')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "# Generate a 100x100 grid of random integers (for demonstration purposes)\n",
    "# Let's assume values range from 1 to 100 for simplicity\n",
    "grid = base_grid\n",
    "\n",
    "# Perform a simplified \"transform\" by summing values along one dimension (akin to reducing dimensionality)\n",
    "transformed_data = np.sum(grid, axis=0)  # Sum along columns\n",
    "\n",
    "# Conceptually, to apply \"periodic summation\" to the transformed data, we would replicate this data\n",
    "# Here, we simulate this by replicating the transformed data three times to illustrate periodicity\n",
    "periodic_summation_example = np.tile(transformed_data, 55)  # Replicate 3 times for illustration\n",
    "\n",
    "# Show the shape of the original, transformed, and \"periodically summed\" data\n",
    "original_shape = grid.shape\n",
    "transformed_shape = transformed_data.shape\n",
    "periodic_summed_shape = periodic_summation_example.shape\n",
    "\n",
    "original_shape, transformed_shape, periodic_summed_shape, periodic_summation_example[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate a 100x100 grid of random integers\n",
    "np.random.seed(0)  # For reproducibility\n",
    "grid = np.random.randint(1, 101, size=(100, 100))\n",
    "\n",
    "# Step 2: Transform the data by summing along one dimension\n",
    "transformed_data = np.sum(grid, axis=0)  # Sum along columns\n",
    "\n",
    "# Step 3: Apply \"periodic summation\" by replicating the transformed data\n",
    "periodic_summation_example = np.tile(transformed_data, 3)  # Replicate 3 times\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# Original Grid Visualization\n",
    "axs[0].imshow(grid, cmap='viridis')\n",
    "axs[0].set_title('Original 100x100 Grid')\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Transformed Data Visualization\n",
    "axs[1].plot(transformed_data)\n",
    "axs[1].set_title('Transformed Data (Summed along one dimension)')\n",
    "axs[1].set_xlabel('Index')\n",
    "axs[1].set_ylabel('Summed Value')\n",
    "\n",
    "# Periodically Summed Data Visualization\n",
    "axs[2].plot(periodic_summation_example)\n",
    "axs[2].set_title('Periodically Summed Data')\n",
    "axs[2].set_xlabel('Index')\n",
    "axs[2].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Output the shapes for clarity\n",
    "original_shape, transformed_shape, periodic_summed_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense neural networks\n",
    "\n",
    "https://medium.com/@karuneshu21/implement-densenet-in-pytorch-46374ef91900\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with tahn and applying L2 regularization\n",
    "\n",
    "Creating and conducting a test similar to the one described involves a structured approach that includes designing a neural network with specific features such as skip connections, using a `tanh` activation function, applying L2 regularization, and incorporating rotation transformations during the training process. This experiment demonstrates the application of concepts from quantum mechanics (like rotation transformations) in a deep learning context to potentially enhance the learning process or model performance. Let's break down the steps required to set up and conduct this experiment using Python and TensorFlow, a popular deep learning library.\n",
    "\n",
    "### Step 1: Environment Setup\n",
    "\n",
    "Before starting, ensure Python is installed on your machine along with TensorFlow, which will be used to build and train the neural network model. The installation can be done using pip, Python's package installer, by running the command below in your terminal or command prompt:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow numpy\n",
    "```\n",
    "\n",
    "This command installs TensorFlow and NumPy, a library for numerical computations that's often used alongside TensorFlow for handling arrays and matrices.\n",
    "\n",
    "### Step 2: Create the Neural Network Model\n",
    "\n",
    "The neural network model is designed with three layers, incorporating skip connections between layers to potentially enhance information flow and mitigate issues like vanishing gradients. The `tanh` activation function is used for its non-linear properties, allowing the model to learn complex patterns. L2 regularization is applied to each layer to prevent overfitting by penalizing large weights.\n",
    "\n",
    "Here's how you can define such a model in TensorFlow:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    skip1 = layers.Add()([x, inputs])\n",
    "\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip1)\n",
    "    skip2 = layers.Add()([x, skip1])\n",
    "\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip2)\n",
    "    output = layers.Add()([x, skip2])\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "```\n",
    "\n",
    "### Step 3: Training Setup\n",
    "\n",
    "To train the model, you'll need a dataset. In this experiment, synthetic data is used, where both input `x` and target `y` are random vectors of the same dimension. This setup allows for exploring the model's capacity to learn complex mappings from inputs to outputs without focusing on real-world data intricacies.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "input_shape = 20  # Example input shape\n",
    "x_train = np.random.rand(num_samples, input_shape)\n",
    "y_train = np.random.rand(num_samples, input_shape)\n",
    "```\n",
    "\n",
    "### Step 4: Initial Training\n",
    "\n",
    "Train the model with the synthetic dataset using gradient descent. The `adam` optimizer is chosen for its efficiency and adaptability in various scenarios, and mean squared error is used as the loss function, suitable for regression problems like predicting continuous values.\n",
    "\n",
    "```python\n",
    "model = create_model(input_shape)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "### Step 5: Apply Rotation Transformations\n",
    "\n",
    "To mimic quantum mechanical rotations, apply rotation transformations to the weight matrices of the model after each training epoch. This step is inspired by quantum mechanics and aims to explore new states (or configurations) of the model that might lead to better performance or faster convergence.\n",
    "\n",
    "```python\n",
    "def apply_rotation(W, H):\n",
    "    return W + np.dot(H, W) - np.dot(W, H)\n",
    "\n",
    "for epoch in range(10):  # Additional training epochs\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=32, verbose=0)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if 'dense' in layer.name:\n",
    "            W = layer.get_weights()[0]\n",
    "            H = np.random.rand(*W.shape)\n",
    "            H = H - H.T  # Make H anti-symmetric\n",
    "            W_rotated = apply_rotation(W, H * 0.01)\n",
    "            layer.set_weights([W_rotated, layer.get_weights()[1]])\n",
    "```\n",
    "\n",
    "### Step 6: Evaluation and Interpretation\n",
    "\n",
    "After the training process, including the application of rotation transformations, evaluate the model's performance on unseen data (test data) to assess its generalization ability. Additionally, analyzing the model's behavior, such as how the weights change over training epochs due to the rotation transformations, can offer insights into the learning dynamics and the effects of the applied quantum mechanics-inspired techniques.\n",
    "\n",
    "This experiment serves as a simplified illustration of how concepts from quantum mechanics can be applied in deep learning. The actual implementation and effectiveness of such techniques can vary depending on the complexity of the data and the specific problem\n",
    "\n",
    "being addressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "\n",
    "# Create model\n",
    "def create_model(input_shape):\n",
    "    inputs = layers.Input(shape=(input_shape,))  # Note the comma to make it a tuple\n",
    "\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    skip1 = layers.Add()([x, inputs])\n",
    "\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip1)\n",
    "    skip2 = layers.Add()([x, skip1])\n",
    "\n",
    "    x = layers.Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01))(skip2)\n",
    "    output = layers.Add()([x, skip2])\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "base_grid_flat = base_grid.to_numpy().flatten()\n",
    "# Generate a 100x100 grid of random integers (for demonstration purposes)\n",
    "# Let's assume values range from 1 to 100 for simplicity\n",
    "grid = base_grid\n",
    "\n",
    "\n",
    "\n",
    "num_samples = 10000\n",
    "input_shape = 64  # Example input shape\n",
    "x_train = grid\n",
    "y_train = np.random.rand(num_samples, input_shape)\n",
    "\n",
    "model = create_model(input_shape)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "def apply_rotation(W, H):\n",
    "    return W + np.dot(H, W) - np.dot(W, H)\n",
    "\n",
    "for epoch in range(10):  # Additional training epochs\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=32, verbose=0)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if 'dense' in layer.name:\n",
    "            W = layer.get_weights()[0]\n",
    "            H = np.random.rand(*W.shape)\n",
    "            H = H - H.T  # Make H anti-symmetric\n",
    "            W_rotated = apply_rotation(W, H * 0.01)\n",
    "            layer.set_weights([W_rotated, layer.get_weights()[1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list of numbers\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "array = np.array(numbers)\n",
    "\n",
    "# Calculate the sum of squares\n",
    "sum_of_squares = np.sum(array ** 2)\n",
    "\n",
    "print(sum_of_squares)  # Output: 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "sum_of_squares = sum(x ** 2 for x in numbers)\n",
    "\n",
    "print(sum_of_squares)  # Output: 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[1.1, 2.1], [3.2, 4.2]])\n",
    "\n",
    "# Calculate the Frobenius norm of the difference\n",
    "frobenius_norm = np.linalg.norm(A - B, 'fro')\n",
    "\n",
    "print(frobenius_norm)  # Output: 0.4472135954999579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, if A and B are two matrices of the same size, the Frobenius norm of their difference is calculated as:\n",
    "||A - B||\\_F = sqrt(sum((A - B) \\*\\* 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[1.1, 2.1], [3.2, 4.2]])\n",
    "\n",
    "# Calculate the Frobenius norm of the difference\n",
    "frobenius_norm = np.linalg.norm(A - B, 'fro')\n",
    "\n",
    "print(frobenius_norm)  # Output: 0.4472135954999579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# create base grid\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid\n",
    "# Define two matrices\n",
    "A = grid\n",
    "B = np.array([[1.1, 2.1], [3.2, 4.2]])\n",
    "\n",
    "# Calculate the Frobenius norm of the difference\n",
    "frobenius_norm = np.linalg.norm(A - B, 'fro')\n",
    "\n",
    "print(frobenius_norm)  # Output: 0.4472135954999579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitution(input_block, sbox):\n",
    "    \"\"\"Apply S-box substitution on the input block.\"\"\"\n",
    "    output = 0\n",
    "    for i in range(0, len(input_block)):\n",
    "        # Extract the corresponding bit group and apply the S-box\n",
    "        output |= sbox[input_block[i]] << (4 * i)\n",
    "    return output\n",
    "\n",
    "def permutation(input_block, pbox):\n",
    "    \"\"\"Apply P-box permutation on the input block.\"\"\"\n",
    "    output = 0\n",
    "    for i, p in enumerate(pbox):\n",
    "        bit = (input_block >> i) & 1\n",
    "        output |= bit << p\n",
    "    return output\n",
    "\n",
    "def key_mixing(input_block, subkey):\n",
    "    \"\"\"Mix the input block with the subkey using XOR.\"\"\"\n",
    "    return input_block ^ subkey\n",
    "\n",
    "def spn_encrypt(plaintext, keys, sbox, pbox):\n",
    "    \"\"\"Encrypt using a simple SPN.\"\"\"\n",
    "    state = plaintext\n",
    "    for key in keys[:-1]:  # Last key is used after the final round\n",
    "        state = substitution(state, sbox)\n",
    "        state = permutation(state, pbox)\n",
    "        state = key_mixing(state, key)\n",
    "    # Apply the final round (without permutation)\n",
    "    state = substitution(state, sbox)\n",
    "    state = key_mixing(state, keys[-1])\n",
    "    return state\n",
    "\n",
    "# Example S-box and P-box definitions\n",
    "sbox = [0xE, 0x4, 0xD, 0x1, 0x2, 0xF, 0xB, 0x8, 0x3, 0xA, 0x6, 0xC, 0x5, 0x9, 0x0, 0x7]\n",
    "pbox = \n",
    "\n",
    "# Example keys for each round (assuming a 4-round SPN)\n",
    "keys = [0x1234, 0x5678, 0x9abc, 0xdef0, 0x1111]\n",
    "\n",
    "# Encrypting a 16-bit plaintext\n",
    "plaintext = 0x1234\n",
    "ciphertext = spn_encrypt(plaintext, keys, sbox, pbox)\n",
    "\n",
    "print(f'Plaintext: 0x{plaintext:04x}')\n",
    "print(f'Ciphertext: 0x{ciphertext:04x}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the concepts of Lehmer codes, inversion tables, Rothe diagrams, and generating permutations as described, let's break down the explanation into code examples and visual representations for better understanding. We will use a specific permutation as an example, as you mentioned,  = (6, 3, 8, 1, 4, 9, 7, 2, 5), and illustrate how to work with Lehmer codes, inversion tables, and Rothe diagrams in Python.\n",
    "\n",
    "### 1. Representing Permutations with Lehmer Codes\n",
    "\n",
    "First, let's discuss how to convert a permutation into its Lehmer code and back. The Lehmer code of a permutation gives a way to encode permutations compactly, and it's useful for generating permutations efficiently.\n",
    "\n",
    "```python\n",
    "def permutation_to_lehmer(permutation):\n",
    "    lehmer_code = []\n",
    "    for i in range(len(permutation)):\n",
    "        lehmer_code.append(sum(1 for j in permutation[i+1:] if j < permutation[i]))\n",
    "    return lehmer_code\n",
    "\n",
    "def lehmer_to_permutation(lehmer_code):\n",
    "    n = len(lehmer_code)\n",
    "    permutation = list(range(1, n+1))\n",
    "    for i in range(n):\n",
    "        j = lehmer_code[i]\n",
    "        permutation[i], permutation[i+j] = permutation[i+j], permutation[i]\n",
    "    return permutation\n",
    "```\n",
    "\n",
    "### 2. Visualizing Rothe Diagrams\n",
    "\n",
    "A Rothe diagram is a grid representation of a permutation, showing dots at (i, i) and crosses for each inversion. Here's a basic way to visualize a Rothe diagram using text:\n",
    "\n",
    "```python\n",
    "def print_rothe_diagram(permutation):\n",
    "    n = len(permutation)\n",
    "    diagram = [[' ' for _ in range(n)] for _ in range(n)]\n",
    "    for i, val in enumerate(permutation):\n",
    "        diagram[i][val-1] = ''  # Mark the position of the permutation\n",
    "        for x in range(val-1):\n",
    "            for y in range(i+1, n):\n",
    "                diagram[y][x] = 'x'  # Mark the inversions\n",
    "\n",
    "    for row in diagram:\n",
    "        print(' '.join(row))\n",
    "```\n",
    "\n",
    "### 3. Generating Permutations\n",
    "\n",
    "#### Generating All Permutations in Lexicographic Order\n",
    "\n",
    "Python's `itertools.permutations` function already generates all permutations of a given sequence in lexicographic order if the sequence is sorted. However, to manually implement this for learning purposes:\n",
    "\n",
    "```python\n",
    "from itertools import permutations\n",
    "\n",
    "# Generate all permutations of a sequence\n",
    "sequence = [1, 2, 3]\n",
    "all_perms = list(permutations(sequence))\n",
    "print(all_perms)\n",
    "```\n",
    "\n",
    "#### Converting Natural Numbers to Permutations\n",
    "\n",
    "To convert natural numbers to permutations, we can use the factorial number system. Here's a simple example to convert a number to a permutation of `[1, 2, 3]`:\n",
    "\n",
    "```python\n",
    "def number_to_permutation(number, n):\n",
    "    # Assuming n = 3 for simplicity\n",
    "    factorial_bases = [2, 1]  # For n=3, skipping the last base as it's always 0\n",
    "    digits = []\n",
    "\n",
    "    for base in factorial_bases:\n",
    "        digits.append(number // base)\n",
    "        number = number % base\n",
    "\n",
    "    # Convert digits to permutation\n",
    "    items = list(range(1, n+1))\n",
    "    permutation = []\n",
    "    for digit in digits:\n",
    "        permutation.append(items.pop(digit))\n",
    "    permutation.append(items[0])  # Add the last item\n",
    "    return permutation\n",
    "```\n",
    "\n",
    "### Visualizing and Understanding\n",
    "\n",
    "By translating these concepts into code, you can experiment with and visualize permutations, their Lehmer codes, and Rothe diagrams directly. This hands-on approach aids in understanding the compact representation of permutations, their systematic generation, and the interesting properties of their encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the base grid from a CSV file\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid\n",
    "\n",
    "def permutation_to_lehmer(permutation):\n",
    "    lehmer_code = []\n",
    "    for i in range(len(permutation)):\n",
    "        lehmer_code.append(sum(1 for j in permutation[i+1:] if j < permutation[i]))\n",
    "    return lehmer_code\n",
    "\n",
    "def lehmer_to_permutation(lehmer_code):\n",
    "    n = len(lehmer_code)\n",
    "    permutation = list(range(1, n+1))\n",
    "    for i in range(n):\n",
    "        j = lehmer_code[i]\n",
    "        permutation[i], permutation[i+j] = permutation[i+j], permutation[i]\n",
    "    return permutation\n",
    "\n",
    "# Example permutation\n",
    "permutation = [3, 1, 4, 5, 2]\n",
    "\n",
    "# Convert the permutation to its Lehmer code\n",
    "lehmer_code = permutation_to_lehmer(permutation)\n",
    "\n",
    "# Convert the Lehmer code back to the original permutation\n",
    "reconstructed_permutation = lehmer_to_permutation(lehmer_code)\n",
    "\n",
    "# Output the original permutation and the reconstructed permutation\n",
    "print(f'Original permutation: {permutation}')\n",
    "print(f'Reconstructed permutation: {reconstructed_permutation}')\n",
    "print(f'Lehmer code: {lehmer_code}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the base grid from a CSV file\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid\n",
    "\n",
    "\n",
    "def print_rothe_diagram(permutation):\n",
    "    n = len(permutation)\n",
    "    diagram = [[' ' for _ in range(n)] for _ in range(n)]\n",
    "    for i, val in enumerate(permutation):\n",
    "        diagram[i][val-1] = ''  # Mark the position of the permutation\n",
    "        for x in range(val-1):\n",
    "            for y in range(i+1, n):\n",
    "                diagram[y][x] = 'x'  # Mark the inversions\n",
    "\n",
    "    for row in diagram:\n",
    "        print(' '.join(row))\n",
    "        \n",
    "# Example permutation\n",
    "permutation = base_grid[0]\n",
    "\n",
    "# Print the Rothe diagram for the permutation\n",
    "print_rothe_diagram(permutation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "# Load the base grid from a CSV file\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid\n",
    "\n",
    "\n",
    "\n",
    "# Generate all permutations of a sequence\n",
    "sequence = [1, 2, 3]\n",
    "all_perms = list(permutations(sequence))\n",
    "print(all_perms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the base grid from a CSV file\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid\n",
    "\n",
    "def number_to_permutation(number, n):\n",
    "    # Assuming n = 3 for simplicity\n",
    "    factorial_bases = [2, 1]  # For n=3, skipping the last base as it's always 0\n",
    "    digits = []\n",
    "\n",
    "    for base in factorial_bases:\n",
    "        digits.append(number // base)\n",
    "        number = number % base\n",
    "\n",
    "    # Convert digits to permutation\n",
    "    items = list(range(1, n+1))\n",
    "    permutation = []\n",
    "    for digit in digits:\n",
    "        permutation.append(items.pop(digit))\n",
    "    permutation.append(items[0])  # Add the last item\n",
    "    return permutation\n",
    "\n",
    "# Example number and permutation length\n",
    "number = 5\n",
    "n = 3\n",
    "\n",
    "# Convert the number to a permutation\n",
    "permutation = number_to_permutation(number, n)\n",
    "\n",
    "# Output the original number and the reconstructed permutation\n",
    "print(f'Original number: {number}')\n",
    "print(f'Reconstructed permutation: {permutation}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the base grid from a CSV file\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid\n",
    "\n",
    "\n",
    "def sjt_permutations(n):\n",
    "    \"\"\"Generate permutations of n elements in Gray code order.\"\"\"\n",
    "    # Initial permutation\n",
    "    p = list(range(1, n + 1))\n",
    "    # Direction of each element (left: -1, right: +1)\n",
    "    dir = [-1] * n\n",
    "    \n",
    "    yield p[:]\n",
    "    \n",
    "    while True:\n",
    "        # Find the largest mobile element (an element that can be swapped in its direction)\n",
    "        mobile = None\n",
    "        mobile_index = None\n",
    "        for i in range(n):\n",
    "            if dir[i] == -1 and i > 0 and p[i] > p[i-1] or \\\n",
    "               dir[i] == 1 and i < n-1 and p[i] > p[i+1]:\n",
    "                if mobile is None or p[i] > p[mobile]:\n",
    "                    mobile = i\n",
    "        \n",
    "        if mobile is None:  # No mobile element means we're done\n",
    "            return\n",
    "        \n",
    "        # Swap the mobile element in its direction\n",
    "        swap_with = mobile + dir[mobile]\n",
    "        p[mobile], p[swap_with] = p[swap_with], p[mobile]\n",
    "        dir[mobile], dir[swap_with] = dir[swap_with], dir[mobile]\n",
    "        \n",
    "        # Reverse the direction of all elements larger than the current mobile element\n",
    "        for i in range(n):\n",
    "            if p[i] > p[mobile]:\n",
    "                dir[i] *= -1\n",
    "        \n",
    "        yield p[:]\n",
    "\n",
    "# Example usage\n",
    "n = 4\n",
    "for perm in sjt_permutations(n):\n",
    "    print(perm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29632\\3675215153.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the base grid from a CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbase_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../storage/a.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpermutation_to_factorial_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bradl\\OneDrive\\Documents\\GitHub\\raft-in-water\\2d_game_backend\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6293\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6294\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6296\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the base grid from a CSV file\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid.flatten()\n",
    "\n",
    "def permutation_to_factorial_number(permutation):\n",
    "    n = len(permutation)\n",
    "    number = 0\n",
    "    for i in range(n):\n",
    "        inversions = sum(1 for j in range(i+1, n) if permutation[j] < permutation[i])\n",
    "        number += inversions * math.factorial(n - i - 1)\n",
    "    return number\n",
    "\n",
    "# Example\n",
    "permutation = grid\n",
    "number = permutation_to_factorial_number(permutation)\n",
    "print(f\"The permutation {permutation} corresponds to the number {number} in the factorial number system.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The permutation [3, 1, 4, 5, 2] corresponds to the number 51 in the factorial number system.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the base grid from a CSV file\n",
    "base_grid = pd.read_csv('../storage/a.csv', header=None)\n",
    "grid = base_grid\n",
    "\n",
    "def permutation_to_factorial_number(permutation):\n",
    "    n = len(permutation)\n",
    "    number = 0\n",
    "    for i in range(n):\n",
    "        inversions = sum(1 for j in range(i+1, n) if permutation[j] < permutation[i])\n",
    "        number += inversions * math.factorial(n - i - 1)\n",
    "    return number\n",
    "\n",
    "# Example\n",
    "permutation = [3, 1, 4, 5, 2]\n",
    "number = permutation_to_factorial_number(permutation)\n",
    "print(f\"The permutation {permutation} corresponds to the number {number} in the factorial number system.\")\n",
    "\n",
    "# define a function to convert a number to a permutation\n",
    "# display the generated permutation\n",
    "# convert the permutation back to a number\n",
    "# display the original number and the reconstructed permutation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
